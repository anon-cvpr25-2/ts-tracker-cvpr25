{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":363867,"status":"ok","timestamp":1713284278431,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"m1DelAryz1sX","outputId":"15e36037-4337-48e6-8ae6-25c1cfa70869"},"outputs":[],"source":["#@title Installing necessary libraries, that are not already installed on Colab\n","# This might take a couple of minutes, and it may restart the runtime!\n","# If the runtime requests reloading, canceling is usually the better.\n","# (This might change in the future.)\n","\n","# Detectron2 environment for instance segmentation\n","!pip install \"git+https://github.com/facebookresearch/detectron2.git\"\n","\n","# SMP environment for tracking\n","!pip install segmentation-models-pytorch"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3580,"status":"ok","timestamp":1713285720359,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"0m7ULgYGxRC3"},"outputs":[],"source":["#@title Importing required functions and libraries\n","\n","# Imports from Symmetry-Tracker repo\n","\n","import sys\n","sys.path.append(\"/path/to/symmetry_tracker/\")\n","\n","from symmetry_tracker.general_functionalities.video_transformation import TransformVideoFromTIFF\n","\n","from symmetry_tracker.segmentation.segmentator import SingleVideoSegmentation\n","from symmetry_tracker.segmentation.segmentation_io import DisplaySegmentation, WriteSegmentation\n","\n","from symmetry_tracker.tracking.tracker import SingleVideoObjectTracking\n","from symmetry_tracker.tracking.tracking_io import DisplayTracks, WriteTracks, SaveTracksVideo\n","from symmetry_tracker.tracking.post_processing import InterpolateMissingObjects\n","\n","# Other necessary imports\n","\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16470,"status":"ok","timestamp":1713285738070,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"_NUdw1BIyZvZ","outputId":"aa31f9f7-3a80-4de7-8cb0-3406c83fa9b5"},"outputs":[],"source":["#@title Downloading the Models and Sample Data\n","\n","import os\n","\n","!mkdir downloads\n","\n","segmentator_models_dir = \"/path/to/models/\"\n","segmentator_model_name = \"model_final.pth\"\n","segmentator_config_name = \"config.yaml\"\n","\n","# Tracking model\n","tracking_models_dir = \"/path/to/models/\"\n","tracking_model_name = \"MOTSynth_KernelTracker_[DLV3p,resnet50]_FBtr2_Ep4_Adv2_final.pth\"\n","\n","# Sample recording\n","sample_record_name = \"600\"\n","sample_records_dir = f\"/path/to/transformed/data/recordings/frames/{sample_record_name}/rgb/\"\n","\n","!wget --no-clobber $segmentator_models_dir$segmentator_model_name -P downloads/\n","!wget --no-clobber $segmentator_models_dir$segmentator_config_name -P downloads/\n","!wget --no-clobber $tracking_models_dir$tracking_model_name -P downloads/\n","\n","!wget -r -nd -np -A jpg,jpeg,png $sample_records_dir -P downloads/sample_images/$sample_record_name/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":726,"status":"ok","timestamp":1713285742550,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"1J1dSslcOO-t","outputId":"461c770a-bee2-4519-afb3-d2b8f5f805bc"},"outputs":[],"source":["#@title Pipeline parameter setup\n","\n","# Input paths\n","SegmentationModelPath = \"./downloads/\"+segmentator_model_name\n","SegmentationModelConfigPath = \"./downloads/\"+segmentator_config_name\n","TrackingModelPath = \"./downloads/\"+tracking_model_name\n","\n","!mkdir inputs\n","!mkdir inputs/frames\n","TransformedVideoPath = \"./inputs/frames/\"\n","\n","# Output paths\n","!mkdir outputs\n","!mkdir outputs/segmentations\n","!mkdir outputs/trackings\n","!mkdir outputs/videos\n","SegmentationSavePath = \"./outputs/segmentations/\"+sample_record_name+\"_Segmentation.txt\"\n","TrackingSavePath = \"./outputs/trackings/\"+sample_record_name+\"_Tracks.json\"\n","TrackingWritePath = \"./outputs/trackings/\"+sample_record_name+\"_Tracks.txt\"\n","TrackingVideoPath = \"./outputs/videos/\"+sample_record_name+\"_Tracks.mp4\"\n","\n","# TimeKernelSize is the size of the kernel in both directions without the central image\n","# (So if TimeKernelSize is 2, then the full kernel size is 5)\n","# This parameter is unique to the given tracking model\n","TimeKernelSize = 2\n","\n","# Matching colab environment (for now GPU vs CPU)\n","Device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Colab environment: \"+Device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":483,"status":"ok","timestamp":1713284549427,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"8UJaCdqOZMLX"},"outputs":[],"source":["#@title Video Transformation\n","\n","import shutil\n","\n","max_sample_num = None\n","\n","shutil.rmtree(TransformedVideoPath)\n","os.mkdir(TransformedVideoPath)\n","\n","all_sample_names = sorted(os.listdir(f\"downloads/sample_images/{sample_record_name}/\"))\n","for sample_pos in range(len(all_sample_names)):\n","  if max_sample_num is None or sample_pos < max_sample_num:\n","    shutil.copy(os.path.join(f\"downloads/sample_images/{sample_record_name}/\", all_sample_names[sample_pos]),\n","                TransformedVideoPath)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","executionInfo":{"elapsed":2,"status":"ok","timestamp":1713270281138,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"iM1tjlXWDIf4"},"outputs":[],"source":["#@title Segmentation Modifications\n","\n","import numpy as np\n","import cv2\n","import os\n","from pycocotools import mask as coco_mask\n","from detectron2.config import get_cfg\n","from detectron2.engine import DefaultPredictor\n","\n","try:\n","  from IPython.display import display\n","  from symmetry_tracker.general_functionalities.misc_utilities import progress\n","except:\n","  pass\n","\n","def PerformSegmentation(Predictor, VideoPath, Color = \"GRAYSCALE\", MinObjectSize = None):\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  Outmasks = {}\n","\n","  NumFrames = len(VideoFrames)\n","  print(\"Performing segmentation\")\n","  try:\n","    ProgressBar = display(progress(0, NumFrames), display_id=True)\n","  except:\n","    pass\n","  for Frame in range(0,NumFrames):\n","    try:\n","      ProgressBar.update(progress(Frame, NumFrames))\n","    except:\n","      pass\n","    if Color == \"GRAYSCALE\":\n","      Img = cv2.imread(os.path.join(VideoPath,VideoFrames[Frame]), cv2.IMREAD_GRAYSCALE)\n","      Img = np.expand_dims(Img, axis=2)\n","    elif Color == \"RGB\":\n","      Img = cv2.cvtColor(cv2.imread(os.path.join(VideoPath, VideoFrames[Frame])), cv2.COLOR_BGR2RGB)\n","    else:\n","      raise Exception(f\"{Color} is an invalid keyword for Color\")\n","\n","    Outputs = Predictor(Img)\n","    Outmask = (Outputs[\"instances\"].pred_masks.to(\"cpu\").numpy())\n","    ReducedOutmask = []\n","    for Segment in Outmask:\n","      if MinObjectSize is None or np.sum(Segment) >= MinObjectSize:\n","        ReducedOutmask.append(coco_mask.encode(np.asfortranarray(Segment)))\n","    Outmasks[Frame] = ReducedOutmask\n","  try:\n","    ProgressBar.update(progress(1, 1))\n","  except:\n","    pass\n","  print(\"Segmentation finished\")\n","  return Outmasks\n","\n","def SingleVideoSegmentation(VideoPath, ModelPath, ModelConfigPath, Device, Color = \"GRAYSCALE\", ScoreThreshold = 0.2, MinObjectSize = None):\n","  \"\"\"\n","  - VideoPath: The path to the video (in .png images format) to be segmented\n","  - ModelPath: The path to the model description file (.pth)\n","  - ModelConfigPath: The path to the model configuration file (.yaml)\n","  - Device: The device on which the segmentator should run (cpu or cuda:0)\n","  - Color: A keyword specific to the used model on the color encoding\n","           Available options: GRAYSCALE and RGB\n","  - ScoreThreshold: The acceptance threshold defining the object-ness of a given pixel\n","                    Lower values are more accepting\n","  - MinObjectSize: An optional parameter defining the minimal required object size in terms of pixel area\n","                  None is the default value\n","  \"\"\"\n","  if not Color in [\"GRAYSCALE\", \"RGB\"]:\n","    raise Exception(f\"{Color} is an invalid keyword for Color\")\n","\n","  cfg = get_cfg()\n","  cfg.merge_from_file(ModelConfigPath)\n","  cfg.MODEL.DEVICE = Device\n","  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST=ScoreThreshold\n","  cfg.MODEL.WEIGHTS = ModelPath\n","  Predictor = DefaultPredictor(cfg)\n","  Outmasks = PerformSegmentation(Predictor, VideoPath, Color, MinObjectSize)\n","  return Outmasks"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","executionInfo":{"elapsed":2,"status":"ok","timestamp":1713270281138,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"cbnRIOrFGJIi"},"outputs":[],"source":["#@title Segmentation_io Modifications\n","\n","import numpy as np\n","import cv2\n","import os\n","from pycocotools import mask as coco_mask\n","import matplotlib.pyplot as plt\n","\n","def DisplaySegmentation(VideoPath, Outmasks, DisplayFrameNumber = True, Figsize=(4,4)):\n","  \"\"\"\n","  Displays the segmentation in Outmasks onto the video at VideoPath\n","\n","  - VideoPath: The video to be displayed in standard .png images format\n","  - Outmasks: The segmentation results coming from SingleVideoSegmentation()\n","  - DisplayFrameNumber: Boolean variable marking whether to display the frame number\n","  - Figsize: The shape of the figure in standard matplotlib format\n","  \"\"\"\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  for Frame in range(len(VideoFrames)):\n","    fig, (ax1) = plt.subplots(1, 1, figsize=Figsize)\n","    VideoFrame = cv2.cvtColor(cv2.imread(os.path.join(VideoPath, VideoFrames[Frame])), cv2.COLOR_BGR2RGB)\n","    ax1.imshow(VideoFrame, interpolation='nearest')\n","    if Frame in Outmasks.keys():\n","      SegmentsSum = np.zeros(np.shape(VideoFrame)[0:2])\n","      for segment in Outmasks[Frame]:\n","        SegmentsSum += coco_mask.decode(segment)\n","      ax1.imshow(SegmentsSum>0, cmap=plt.cm.hot, vmax=2, alpha=.3, interpolation='bilinear')\n","      if DisplayFrameNumber:\n","        ax1.text(3, 20, \"Frame \"+str(Frame+1), color=\"deepskyblue\")\n","    plt.show()\n","\n","def WriteSegmentation(Outmasks, SavePath):\n","  \"\"\"\n","  Saves the segmentation to a txt file\n","\n","  - Outmasks: The segmentation results coming from SingleVideoSegmentation()\n","  - SavePath: The path to the file, where the segmentation will be saved\n","                          If the file does not exists, it will be created\n","  \"\"\"\n","  with open(SavePath, \"w\") as OutFile:\n","    OutFile.write(\"POS\\tF\\tCELLNUM\\tX\\tY\\n\")\n","    for Frame in Outmasks:\n","      CellCount=1\n","      for segment in Outmasks[Frame]:\n","        contours, hierarchy  = cv2.findContours(np.array(coco_mask.decode(segment), dtype = np.uint8) ,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n","        for contour in contours:\n","          for a in contour:\n","            l=a[0]\n","            OutFile.write(\"1\"+\"\\t\"+str(Frame+1)+\"\\t\"+\"{:04d}\".format(Frame+1)+\"{:04d}\".format(CellCount)+\"\\t\"+str(l[0])+\"\\t\"+str(l[1])+\"\\n\")\n","          CellCount+=1\n","  print(\"Segmentation saved to: \"+SavePath)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"elapsed":935210,"status":"ok","timestamp":1713271216347,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"ntjAssFgXCKl","outputId":"b9114ef2-18d9-4f21-d49d-41c9f196d0b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performing segmentation\n"]},{"data":{"text/html":["\n","      <progress\n","        value='1'\n","        max='1',\n","        style='width: 10%'\n","      >\n","        1\n","      </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"name":"stdout","output_type":"stream","text":["Segmentation finished\n","Segmentation saved to: ./outputs/segmentations/600_Segmentation.txt\n"]}],"source":["#@title Instance Segmentation\n","\n","# Performing segmentation\n","Outmasks = SingleVideoSegmentation(TransformedVideoPath, SegmentationModelPath, SegmentationModelConfigPath, Device, Color = \"RGB\", ScoreThreshold = 0.4)\n","\n","# Displaying segmentation results\n","#DisplaySegmentation(TransformedVideoPath, Outmasks)\n","\n","# Saving segmentation\n","WriteSegmentation(Outmasks, SegmentationSavePath)"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","executionInfo":{"elapsed":4,"status":"ok","timestamp":1713271216347,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"lqPgi6QOAV_k"},"outputs":[],"source":["#@title Tracker Metrics Modifications\n","\n","import numpy as np\n","\n","def TracksIOU(Array1, Array2, dt):\n","  if Array1.shape != Array2.shape:\n","    raise Exception(f\"Dimensions of Array1 {Array1.shape} and Array2 {Array2.shape} must be the same\")\n","\n","  Intersection = np.count_nonzero(np.logical_and(Array1[dt:], Array2[:-dt]))\n","  Union = np.count_nonzero(np.logical_or(Array1[dt:], Array2[:-dt]))\n","\n","  if Union == 0:\n","    return 0\n","  try:\n","    return Intersection / Union\n","  except:\n","    return 0\n","\n","def SegmentationIOU(Bin1, Bin2):\n","  if Bin1.shape != Bin2.shape:\n","    raise Exception(f\"Dimensions of Bin1 {Bin1.shape} and Bin2 {Bin2.shape} must be the same\")\n","\n","  Intersection = np.count_nonzero(np.logical_and(Bin1,Bin2))\n","  Union = np.count_nonzero(np.logical_or(Bin1,Bin2))\n","\n","  if Union == 0:\n","    return 0\n","  try:\n","    return Intersection / Union\n","  except:\n","    return 0\n","\n","def AnnotationOverlap(Annot1, Annot2):\n","  if Annot1.shape != Annot2.shape:\n","    raise Exception(f\"Dimensions of Annot1 {Annot1.shape} and Annot2 {Annot2.shape} must be the same\")\n","\n","  Intersection = np.sum(Annot1 * Annot2)\n","  if Intersection == 0:\n","    return 0\n","\n","  SmallerArea = min([np.sum(Annot1), np.sum(Annot2)])\n","\n","  if SmallerArea == 0:\n","    return 0\n","  try:\n","    return Intersection / SmallerArea\n","  except:\n","    return 0"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","executionInfo":{"elapsed":3,"status":"ok","timestamp":1713271216347,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"XSNDlFiSXh1W"},"outputs":[],"source":["#@title Tracker Utilities Modifications\n","\n","import numpy as np\n","import cv2\n","import torch\n","import pandas as pd\n","from pycocotools import mask as coco_mask\n","\n","from symmetry_tracker.general_functionalities.misc_utilities import CenterMass, BoxOverlap, BoundingBox\n","#from symmetry_tracker.tracking.tracker_metrics import AnnotationOverlap\n","\n","try:\n","  from IPython.display import display\n","  from symmetry_tracker.general_functionalities.misc_utilities import progress\n","except:\n","  pass\n","\n","\n","def RemoveFaultyObjects(AnnotDF, VideoShape, MinObjectPixelNumber, MaxOverlapRatio):\n","  \"\"\"\n","  Removes the objects from the annotation, which\n","  - have too small size based on self.MinObjectPixelNumber\n","  - have no defineable center\n","  \"\"\"\n","  print(\"Removing faulty object instances\")\n","\n","  Counter = 0\n","  FaultyInstances = {}\n","\n","  NumFrames = len(AnnotDF[\"Frame\"].unique())\n","  try:\n","    ProgressBar = display(progress(0, NumFrames), display_id=True)\n","  except:\n","    pass\n","\n","  for Frame in AnnotDF[\"Frame\"].unique():\n","\n","    try:\n","      ProgressBar.update(progress(Frame, NumFrames))\n","    except:\n","      pass\n","\n","    FaultyInstances[Frame]=[]\n","    ObjectIDs = AnnotDF.query(\"Frame == @Frame\")[\"ObjectID\"]\n","\n","    for ObjectID in ObjectIDs:\n","      ObjectBbox = AnnotDF.query(\"ObjectID == @ObjectID\")[\"SegBbox\"].iloc[0]\n","      ObjectSeg = coco_mask.decode(AnnotDF.query(\"ObjectID == @ObjectID\")[\"SegmentationRLE\"].iloc[0])\n","      Size = np.sum(ObjectSeg)\n","      [Cx,Cy] = AnnotDF.query(\"ObjectID == @ObjectID\")[\"Centroid\"].iloc[0]\n","\n","      HasBetterCoverage = False\n","      for Object2ID in ObjectIDs:\n","        if ObjectID != Object2ID:\n","          Object2Bbox = AnnotDF.query(\"ObjectID == @Object2ID\")[\"SegBbox\"].iloc[0]\n","\n","          if BoxOverlap(ObjectBbox, Object2Bbox) > 0:\n","            Object2Seg = coco_mask.decode(AnnotDF.query(\"ObjectID == @Object2ID\")[\"SegmentationRLE\"].iloc[0])\n","            if np.sum(ObjectSeg) <= np.sum(Object2Seg) and AnnotationOverlap(ObjectSeg, Object2Seg) > MaxOverlapRatio:\n","              HasBetterCoverage = True\n","\n","      if Size<MinObjectPixelNumber or [Cx, Cy]==[None,None] or Cx<=0 or Cy<=0 or Cx>=VideoShape[2]-1 or Cy>=VideoShape[1]-1 or HasBetterCoverage:\n","        FaultyInstances[Frame].append(ObjectID)\n","        Counter+=1\n","\n","  try:\n","    ProgressBar.update(progress(1, 1))\n","  except:\n","    pass\n","\n","  for Frame in FaultyInstances:\n","    for ObjectID in FaultyInstances[Frame]:\n","      AnnotDF = AnnotDF.query(\"ObjectID != @ObjectID\")\n","\n","  print(f\"Number of removed faulty object instances: {Counter}\")\n","  return AnnotDF\n","\n","#ZeroFrame can be 0 or 1 (depending on where the counting starts)\n","def LoadAnnotationDF(AnnotPath, VideoShape, MinObjectPixelNumber = 20, MaxOverlapRatio = 0.2, ZeroFrame = 1, FaultyObjectRemoval = True):\n","  print(\"Loading Annotation from:\")\n","  print(AnnotPath)\n","  AnnotFile = open(AnnotPath, 'r')\n","  AnnotDF = pd.DataFrame(columns = [\"Frame\", \"ObjectID\", \"SegmentationRLE\", \"LocalTrackRLE\",\n","                                    \"Centroid\", \"SegBbox\", \"TrackBbox\", \"PrevID\", \"NextID\", \"TrackID\", \"Interpolated\"])\n","  FirstRow = True\n","  PrevObjectID = -1\n","  PrevFrame = -1\n","  PolyLine = []\n","  NewObjectID = 1\n","  for line in AnnotFile:\n","    if not FirstRow and line:\n","      splitted = line.split()\n","      Frame = int(splitted[1])\n","      ObjectID = splitted[2]\n","      x = int(float(splitted[3]))\n","      y = int(float(splitted[4]))\n","      if (ObjectID != PrevObjectID and PrevObjectID!=-1) or (Frame != PrevFrame and PrevFrame != -1):\n","        IndividualSegImg = np.zeros([VideoShape[1],VideoShape[2]])\n","        cv2.fillPoly(IndividualSegImg,np.int32([PolyLine]),1)\n","        IndividualSegImg = np.array(IndividualSegImg, dtype=bool)\n","        Centroid = CenterMass(IndividualSegImg)\n","        Bbox = BoundingBox(IndividualSegImg)\n","        FullObjectID = \"{:04d}\".format(PrevFrame - ZeroFrame)+\"{:04d}\".format(NewObjectID)\n","        IndividualSegImgRLE = coco_mask.encode(np.asfortranarray(IndividualSegImg))\n","        AnnotRow = pd.Series({\"Frame\": PrevFrame - ZeroFrame, \"ObjectID\": FullObjectID, \"SegmentationRLE\": IndividualSegImgRLE, \"LocalTrackRLE\": None,\n","                      \"Centroid\": Centroid, \"SegBbox\":Bbox, \"TrackBbox\": None, \"PrevID\": None, \"NextID\": None, \"TrackID\": None, \"Interpolated\": False})\n","        AnnotDF = pd.concat([AnnotDF, AnnotRow.to_frame().T], ignore_index=True)\n","        PolyLine = []\n","        NewObjectID += 1\n","        if Frame != PrevFrame:\n","          NewObjectID = 1\n","      PrevObjectID = ObjectID\n","      PrevFrame = Frame\n","      PolyLine.append([x,y])\n","    FirstRow = False\n","\n","  if FaultyObjectRemoval:\n","    AnnotDF = RemoveFaultyObjects(AnnotDF, VideoShape, MinObjectPixelNumber, MaxOverlapRatio)\n","\n","  return AnnotDF\n","\n","def LoadPretrainedModel(ModelPath, Device):\n","  Model = torch.load(ModelPath, map_location = torch.device(Device))\n","  Model.eval()\n","  print(\"Model successfully loaded from:\")\n","  print(ModelPath)\n","  return Model"]},{"cell_type":"code","execution_count":11,"metadata":{"cellView":"form","executionInfo":{"elapsed":3,"status":"ok","timestamp":1713271216347,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"-buFJdl2LTZd"},"outputs":[],"source":["#@title Tracker Modifications\n","\n","import time\n","import cv2\n","import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import gc\n","from scipy.optimize import linear_sum_assignment\n","\n","from symmetry_tracker.general_functionalities.misc_utilities import EncodeMultiRLE, DecodeMultiRLE, OuterBoundingBox, BoxOverlap, dfs\n","#from symmetry_tracker.tracking.tracker_metrics import TracksIOU\n","#from symmetry_tracker.tracking.tracker_utilities import LoadAnnotationDF, LoadPretrainedModel\n","\n","try:\n","  from IPython.display import display\n","  from symmetry_tracker.general_functionalities.misc_utilities import progress\n","except:\n","  pass\n","\n","\n","def KernelTrackCentroid(LocalVideo, VideoShape, Model, Device, SegmentationConfidence, ObjectCenter):\n","  with torch.no_grad():\n","    inputs = LocalVideo\n","    CPImage = np.zeros([VideoShape[1],VideoShape[2]])\n","    [CMy,CMx]=ObjectCenter\n","    CPImage[CMx-2:CMx+2,CMy-2:CMy+2]=255\n","    inputs = np.append(inputs, [CPImage], axis=0)\n","    inputs = np.array(inputs, dtype=float)/255\n","    inputs = torch.Tensor(np.array(inputs))\n","\n","    pad_h = (16 - inputs.shape[1] % 16) % 16\n","    pad_w = (16 - inputs.shape[2] % 16) % 16\n","    inputs = F.pad(inputs, (0, pad_w, 0, pad_h), mode='constant', value=0)\n","\n","    inputs=torch.unsqueeze(inputs, dim=0)\n","\n","    inputs=inputs.to(torch.device(Device))\n","    output = np.array(torch.sigmoid(Model(inputs).cpu()))\n","    output = output>SegmentationConfidence\n","    output = output*1.0\n","    output = np.nan_to_num(output, nan=0.0, posinf=1.0, neginf=0.0)\n","\n","    torch.cuda.empty_cache()\n","\n","    \"\"\"\n","    plt.imshow(output[0, 0, :, :])\n","    plt.show()\n","    \"\"\"\n","  return np.array(output[0], dtype = bool)\n","\n","def KernelTrackBbox(LocalVideo, VideoShape, Model, Device, SegmentationConfidence, ObjectBbox):\n","  with torch.no_grad():\n","    inputs = LocalVideo\n","    BboxImg = np.zeros([VideoShape[1],VideoShape[2]])\n","    [x0, y0, x1, y1] = ObjectBbox\n","    BboxImg[x0:x1,y0:y1]=255\n","    inputs = np.append(inputs, [BboxImg], axis=0)\n","    inputs = np.array(inputs, dtype=float)/255\n","    inputs = torch.Tensor(np.array(inputs))\n","\n","    pad_h = (16 - inputs.shape[1] % 16) % 16\n","    pad_w = (16 - inputs.shape[2] % 16) % 16\n","    inputs = F.pad(inputs, (0, pad_w, 0, pad_h), mode='constant', value=0)\n","\n","    inputs=torch.unsqueeze(inputs, dim=0)\n","\n","    inputs=inputs.to(torch.device(Device))\n","    output = np.array(torch.sigmoid(Model(inputs).cpu()))\n","    output = output>SegmentationConfidence\n","    output = output*1.0\n","    output = np.nan_to_num(output, nan=0.0, posinf=1.0, neginf=0.0)\n","\n","    torch.cuda.empty_cache()\n","\n","    \"\"\"\n","    plt.imshow(output[0, 0, :, :])\n","    plt.show()\n","    \"\"\"\n","  return np.array(output[0], dtype = bool)\n","\n","\n","def LocalTracking(VideoPath, VideoShape, AnnotDF, Model, Device, TimeKernelSize, Color = \"GRAYSCALE\", Marker = \"CENTROID\", SegmentationConfidence = 0.2):\n","\n","  if not Color in [\"GRAYSCALE\", \"RGB\"]:\n","    raise Exception(f\"{Color} is an invalid keyword for Color\")\n","  if not Marker in [\"CENTROID\", \"BBOX\"]:\n","    raise Exception(f\"{Marker} is not an appropriate keyword for Marker\")\n","\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  NumFrames = len(VideoFrames)\n","\n","  print(\"Local Tracking\")\n","  try:\n","    ProgressBar = display(progress(0, NumFrames), display_id=True)\n","  except:\n","    pass\n","\n","  for Frame in range(NumFrames):\n","    ObjectIDs = AnnotDF.query(\"Frame == @Frame\")[\"ObjectID\"]\n","    for ObjectID in ObjectIDs:\n","\n","      # Input image Composition\n","\n","      if Color == \"GRAYSCALE\":\n","        CentralImg = cv2.imread(os.path.join(VideoPath,VideoFrames[Frame]), cv2.IMREAD_GRAYSCALE)\n","        LocalVideo = np.repeat(CentralImg[np.newaxis, ...], 2*TimeKernelSize+1, axis=0)\n","        for dt in range(-TimeKernelSize, TimeKernelSize+1):\n","          if Frame+dt >= 0 and Frame+dt < NumFrames and dt != 0:\n","            LocalVideo[dt+TimeKernelSize] = cv2.imread(os.path.join(VideoPath,VideoFrames[Frame+dt]), cv2.IMREAD_GRAYSCALE)\n","\n","      elif Color == \"RGB\":\n","        CentralImg = cv2.cvtColor(cv2.imread(os.path.join(VideoPath, VideoFrames[Frame])), cv2.COLOR_BGR2RGB)\n","        CentralImg = np.transpose(CentralImg, (2,0,1))\n","        NumReps = 2*TimeKernelSize+1\n","        LocalVideo = np.zeros((3*NumReps,\n","                       np.shape(CentralImg)[1],\n","                       np.shape(CentralImg)[2]),\n","                      dtype=CentralImg.dtype)\n","        for Rep in range(NumReps):\n","          LocalVideo[3*Rep:3*Rep+3] = CentralImg\n","        for dt in range(-TimeKernelSize, TimeKernelSize+1):\n","          if Frame+dt >= 0 and Frame+dt < NumFrames and dt != 0:\n","            LocalImg = cv2.cvtColor(cv2.imread(os.path.join(VideoPath, VideoFrames[Frame+dt])), cv2.COLOR_BGR2RGB)\n","            LocalVideo[3*(dt+TimeKernelSize):3*(dt+TimeKernelSize)+3] = np.transpose(LocalImg, (2,0,1))\n","\n","      else:\n","        raise Exception(f\"{Color} is an invalid keyword for Color\")\n","\n","      # Local Tracking\n","\n","      LocalTrack = None\n","\n","      if Marker == \"CENTROID\":\n","        ObjectCenter = AnnotDF.query(\"ObjectID == @ObjectID\")[\"Centroid\"].iloc[0]\n","        LocalTrack = KernelTrackCentroid(LocalVideo, VideoShape, Model, Device, SegmentationConfidence, ObjectCenter)\n","\n","      elif Marker == \"BBOX\":\n","        ObjectBbox = AnnotDF.query(\"ObjectID == @ObjectID\")[\"SegBbox\"].iloc[0]\n","        LocalTrack = KernelTrackBbox(LocalVideo, VideoShape, Model, Device, SegmentationConfidence, ObjectBbox)\n","\n","      AnnotDF.loc[AnnotDF.query(\"ObjectID == @ObjectID\").index, \"LocalTrackRLE\"] = [EncodeMultiRLE(LocalTrack)]\n","\n","      # 3D Boundary Box calculation\n","\n","      bbox = OuterBoundingBox(LocalTrack)\n","      AnnotDF.loc[AnnotDF.query(\"ObjectID == @ObjectID\").index, \"TrackBbox\"] = [bbox]\n","\n","    try:\n","      ProgressBar.update(progress(Frame, NumFrames))\n","    except:\n","      pass\n","\n","  try:\n","    ProgressBar.update(progress(1, 1))\n","  except:\n","    pass\n","\n","  return AnnotDF\n","\n","\n","def GlobalAssignment(VideoPath, VideoShape, AnnotDF, TimeKernelSize, MinRequiredSimilarity=0.5, MaxTimeKernelShift=None):\n","\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  NumFrames = len(VideoFrames)\n","\n","  if MaxTimeKernelShift is None:\n","    MaxTimeKernelShift=TimeKernelSize*2\n","\n","  print(\"Global Assignment\")\n","\n","  try:\n","    ProgressBar = display(progress(0, len(AnnotDF)), display_id=True)\n","  except:\n","    pass\n","\n","  AnnotDF[[\"PrevID\", \"NextID\"]] = None\n","  for dt in range(1,MaxTimeKernelShift):\n","    for Frame in range(NumFrames-dt):\n","\n","      # Similarity Matrix Calculation\n","\n","      t0 = time.time()\n","\n","      T0_IDs = AnnotDF.query(\"Frame == @Frame and NextID.isnull()\", engine='python')[\"ObjectID\"].tolist()\n","      Tdt_IDs = AnnotDF.query(\"Frame == @Frame+@dt and PrevID.isnull()\", engine='python')[\"ObjectID\"].tolist()\n","\n","      SimilarityMatrix = np.zeros((len(T0_IDs),len(Tdt_IDs)))\n","\n","      for i in range(len(T0_IDs)):\n","        T0_ID = T0_IDs[i]\n","\n","        LTR0 = DecodeMultiRLE(AnnotDF.query(\"ObjectID == @T0_ID\")[\"LocalTrackRLE\"].iloc[0])\n","        for j in range(len(Tdt_IDs)):\n","          Tdt_ID = Tdt_IDs[j]\n","\n","          bbox0 = AnnotDF.query(\"ObjectID == @T0_ID\")[\"TrackBbox\"].iloc[0]\n","          bboxdt = AnnotDF.query(\"ObjectID == @Tdt_ID\")[\"TrackBbox\"].iloc[0]\n","          t00 = time.time()\n","          overlap = BoxOverlap(bbox0, bboxdt)\n","          if overlap != 0:\n","            LTRdt = DecodeMultiRLE(AnnotDF.query(\"ObjectID == @Tdt_ID\")[\"LocalTrackRLE\"].iloc[0])\n","            IOU = TracksIOU(LTR0, LTRdt, dt)\n","            SimilarityMatrix[i, j] = IOU\n","\n","      SimilarityMatrix = SimilarityMatrix * (SimilarityMatrix >= MinRequiredSimilarity)\n","\n","      \"\"\"\n","      sns.heatmap(SimilarityMatrix)\n","      plt.show()\n","      \"\"\"\n","\n","      # Hungarian Method based Assignment\n","\n","      try:\n","        T0_assignedVals, Tdt_assignedVals = linear_sum_assignment(1-SimilarityMatrix)\n","      except:\n","        print(f\"Error in linear_sum_assignment at Frame {Frame} to Frame {Frame+dt}\")\n","        continue\n","\n","      for k in range(len(T0_assignedVals)):\n","        if SimilarityMatrix[T0_assignedVals[k], Tdt_assignedVals[k]] >= MinRequiredSimilarity:\n","          T0_ID = T0_IDs[T0_assignedVals[k]]\n","          Tdt_ID = Tdt_IDs[Tdt_assignedVals[k]]\n","          AnnotDF.loc[AnnotDF.query(\"ObjectID == @T0_ID\").index, \"NextID\"] = Tdt_ID\n","          AnnotDF.loc[AnnotDF.query(\"ObjectID == @Tdt_ID\").index, \"PrevID\"] = T0_ID\n","\n","      t_end = time.time()\n","\n","      \"\"\"\n","      print(f\"dt {dt}, Frame {Frame}, t_full {t_end-t0} s\")\n","      \"\"\"\n","\n","      try:\n","        ProgressBar.update(progress(len(AnnotDF.query(\"not NextID.isnull()\")), len(AnnotDF)))\n","      except:\n","        pass\n","\n","  try:\n","    ProgressBar.update(progress(1, 1))\n","  except:\n","    pass\n","\n","  return AnnotDF\n","\n","\n","def ConnectedIDReduction(AnnotDF):\n","\n","  AnnotDF[\"TrackID\"] = None\n","\n","  # Transforming key-value pair matches to an undirected graph dictionary\n","\n","  EquivalencyGraph = {}\n","  for ObjectID in AnnotDF[\"ObjectID\"].unique():\n","    Neighbors = set()\n","    PrevID = AnnotDF.query(\"ObjectID == @ObjectID\")[\"PrevID\"].iloc[0]\n","    NextID = AnnotDF.query(\"ObjectID == @ObjectID\")[\"NextID\"].iloc[0]\n","    if not PrevID is None:\n","      Neighbors.add(PrevID)\n","    if not NextID is None:\n","      Neighbors.add(NextID)\n","    EquivalencyGraph[ObjectID] = Neighbors\n","\n","  # Creating equivalency sets (everything that is connected is equivavlent)\n","\n","  EquivalencySets = []\n","  for RootCandidate in EquivalencyGraph.keys():\n","    CandidateInEqSets=False\n","    for EqSet in EquivalencySets:\n","      if RootCandidate in EqSet:\n","        CandidateInEqSets=True\n","    if not CandidateInEqSets:\n","      Visited=set()\n","      dfs(Visited, EquivalencyGraph, RootCandidate)\n","      EquivalencySets.append(Visited)\n","\n","  # Generating new (minimal) IDs into the AnnotDF\n","\n","  NewTrackID = 1\n","  for EquivalentIDs in EquivalencySets:\n","    for ObjectID in EquivalentIDs:\n","      AnnotDF.loc[AnnotDF.query(\"ObjectID == @ObjectID\").index, \"TrackID\"] = NewTrackID\n","\n","    NewTrackID += 1\n","\n","  return AnnotDF\n","\n","\n","def SingleVideoObjectTracking(VideoPath, ModelPath, Device, AnnotPath, TimeKernelSize,\n","                              Color = \"GRAYSCALE\", Marker = \"CENTROID\", MinObjectPixelNumber=20, SegmentationConfidence = 0.1,\n","                              MinRequiredSimilarity=0.5, MaxOverlapRatio=0.5, MaxTimeKernelShift=None):\n","  \"\"\"\n","  - VideoPath: The path to video in stardard .png images format on which the tracking will be performed\n","  - ModelPath: The path to the pretrained model (the full model definition, not just the state dictionary)\n","  - Device: The device on which the segmentator should run (cpu or cuda:0)\n","  - AnnotPath: The path to a single annotation (segmentation) belonging to the video at VideoPath\n","  - TimeKernelSize: A constant parameter for the trained Tracker.\n","                    TimeKernelSize is the \"radius\" of the kernel, TimeKernelSize*2+1 is the \"diamater\" of the actual kernel.\n","  - Color: A keyword specific to the used model on the color encoding\n","           Available options: GRAYSCALE and RGB\n","  - Marker: A keyword specific to the used model on how the object to be tracked is marked\n","            Available options: CENTROID and BBOX\n","  - MinObjectPixelNumber: Defines the minimal number of pixels in a Object istance for the instance to be recognised as valid\n","                        Object instances with PixelNumber<MinObjectPixelNumber will be simply deleted during initiation\n","  - SegmentationConfidence: A number in [0,1] or defining the confidence threshold for the segmentation\n","                            Lower values are more allowing. Recommanded values are in the [0.1,0.9] range\n","  - MinRequiredSimilarity: The minimal required similarity based on IOU for two trackings to be possibly counted as belonging to the same Object\n","  - MaxOverlapRatio:  The maximal overlap allowed between annotations in the original annotation.\n","                      Above MaxOverlapRatio, the area-wise smaller Object will be removed.\n","                      Not an important parameter if the segmentation is more or less a partitioning\n","  - MaxTimeKernelShift: The maximal shift allowed between trackings to be recognised as belonging to the same Object\n","                        Minimal possible value: 1\n","                        Maximal possible value: 2*TimeKernelSize\n","                        The default None means maximal possible value\n","                        Usually None is recommended\n","                        Smaller values may result in trackings with more \"breaks\", but possibly fewer errors and slightly faster calculation\n","  \"\"\"\n","\n","  if not Color in [\"GRAYSCALE\", \"RGB\"]:\n","    raise Exception(f\"{Color} is an invalid keyword for Color\")\n","  if not Marker in [\"CENTROID\", \"BBOX\"]:\n","    raise Exception(f\"{Marker} is not an appropriate keyword for Marker\")\n","\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  Img0 = cv2.imread(os.path.join(VideoPath,VideoFrames[0]))\n","  VideoShape = [len(os.listdir(VideoPath)), np.shape(Img0)[0], np.shape(Img0)[1]]\n","  AnnotDF = LoadAnnotationDF(AnnotPath, VideoShape, MinObjectPixelNumber, MaxOverlapRatio)\n","\n","  Model = LoadPretrainedModel(ModelPath, Device)\n","  AnnotDF = LocalTracking(VideoPath, VideoShape, AnnotDF, Model, Device, TimeKernelSize, Color, Marker, SegmentationConfidence)\n","  del Model\n","  gc.collect()\n","\n","  AnnotDF = GlobalAssignment(VideoPath, VideoShape, AnnotDF, TimeKernelSize, MinRequiredSimilarity, MaxTimeKernelShift)\n","\n","  AnnotDF = ConnectedIDReduction(AnnotDF)\n","\n","  return AnnotDF"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1713285848920,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"jKSjSyntx9OP"},"outputs":[],"source":["#@title Misc Utilities Modifications\n","\n","def fig2data(fig):\n","    \"\"\"\n","    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it\n","    @param fig a matplotlib figure\n","    @return a numpy 3D array of RGBA values\n","    \"\"\"\n","    # Create a new canvas for the figure\n","    canvas = fig.canvas\n","    canvas.draw()\n","\n","    # Get the width and height of the canvas\n","    w, h = canvas.get_width_height()\n","\n","    # Get the RGBA buffer from the canvas\n","    buf = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n","    buf.shape = (w, h, 4)\n","\n","    # Roll the ALPHA channel to have it in RGBA mode\n","    buf = np.roll(buf, 3, axis=2)\n","\n","    # Destroy the canvas explicitly to release resources\n","    plt.close(fig)\n","\n","    return buf"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713286325716,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"4EmID7V3Z-W3"},"outputs":[],"source":["#@title Tracker IO Modifications\n","\n","import numpy as np\n","import cv2\n","import os\n","import gc\n","from pycocotools import mask as coco_mask\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import pandas as pd\n","\n","#from symmetry_tracker.general_functionalities.misc_utilities import fig2data\n","\n","try:\n","  from IPython.display import display\n","  from symmetry_tracker.general_functionalities.misc_utilities import progress\n","except:\n","  pass\n","\n","def SaveTracks(AnnotDF, SavePath):\n","  \"\"\"\n","  Saves the AnnotDF dataframe to a json\n","  \"\"\"\n","  if not SavePath.endswith('.json'):\n","    raise ValueError(\"SavePath must have a .json extension\")\n","  AnnotDF.to_json(SavePath, orient='records')\n","\n","def LoadTracks(LoadPath):\n","  \"\"\"\n","  Loads the AnnotDF dataframe from a json\n","  \"\"\"\n","  if not LoadPath.endswith('.json'):\n","    raise ValueError(\"LoadPath must have a .json extension\")\n","  AnnotDF = pd.read_json(LoadPath, orient='records')\n","  return AnnotDF\n","\n","def WriteTracks(AnnotDF, SavePath):\n","  \"\"\"\n","  Saves the optimal annotation to a txt (standard format)\n","  Usually should be performed before interpolating missing cell points, as the interpolated values will be saved as regular (unless this behavior is desired)\n","  \"\"\"\n","  with open(SavePath, \"w\") as OutFile:\n","    OutFile.write(\"POS\\tF\\tCELLNUM\\tX\\tY\\n\")\n","    for Frame in AnnotDF[\"Frame\"].unique():\n","      for TrackID in AnnotDF.query(\"Frame == @Frame\")[\"TrackID\"].unique():\n","        Segmentation = coco_mask.decode(AnnotDF.query(\"TrackID == @TrackID\")[\"SegmentationRLE\"].iloc[0]).astype(np.uint8)\n","        contours, _  = cv2.findContours(Segmentation ,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n","        for contour in contours:\n","          for a in contour:\n","            l=a[0]\n","            OutFile.write(str(-1)+\"\\t\"+str(Frame+1)+\"\\t\"+str(TrackID)+\"\\t\"+str(l[0])+\"\\t\"+str(l[1])+\"\\n\")\n","  print(\"The track is saved in standard txt format to: \")\n","  print(SavePath)\n","\n","def DisplayTracks(VideoPath, AnnotDF, DisplayFrameNumber = True, DisplaySegmentation=True, DisplayTrackIDs = True, DisplayPeriodicity=1, Figsize=(5,5)):\n","  \"\"\"\n","  Displays all cell paths as cell IDs on each frame\n","\n","  - DisplayFrameNumber: Boolean variable marking whether to display the frame number\n","  - DisplaySegmentation: Boolean variable marking whether to display the segmentations\n","                          If True, non-interpolated segments are displayed as \"lime\", interpolated ones are displayed as \"deepskyblue\"\n","  - DisplayTrackIDs: Boolean variable marking whether to display the track IDs\n","  - DisplayPeriodicity: How frequently should the frames be displayed.\n","                        Useful for long videos.\n","                        Minimal possible value=1.\n","  - Figsize: The shape of the figure in standard matplotlib format\n","  \"\"\"\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  for Frame in range(len(VideoFrames)):\n","    if np.mod(Frame,DisplayPeriodicity)==0:\n","\n","      Img = cv2.cvtColor(cv2.imread(os.path.join(VideoPath, VideoFrames[Frame])), cv2.COLOR_BGR2RGB)\n","      fig, (ax1) = plt.subplots(1, 1, figsize=Figsize)\n","      ax1.imshow(Img, cmap=plt.cm.gray, interpolation='nearest')\n","\n","      if DisplaySegmentation:\n","        SegmentsSum = np.zeros(np.shape(Img)[0:2])\n","        for _, Object in AnnotDF.query(\"Frame == @Frame\").iterrows():\n","          SegmentsSum += coco_mask.decode(Object[\"SegmentationRLE\"])\n","        ax1.imshow(SegmentsSum, cmap=plt.cm.hot, vmax=2, alpha=.3, interpolation='bilinear')\n","\n","      if DisplayFrameNumber:\n","        ax1.text(3, 20, \"Frame \"+str(Frame+1), color=\"deepskyblue\")\n","\n","      if DisplayTrackIDs:\n","        for _, Object in AnnotDF.query(\"Frame == @Frame\").iterrows():\n","          color=\"lime\"\n","          if Object[\"Interpolated\"]:\n","            color=\"deepskyblue\"\n","          [cx, cy] = Object[\"Centroid\"]\n","          ax1.text(cx-5, cy+5, Object[\"TrackID\"], color=color)\n","\n","      plt.show()\n","\n","def SaveTracksVideo(VideoPath, AnnotDF, OutputVideoPath,\n","                    Fps = 10, DisplayPeriodicity=1, StartingFrame=None, EndingFrame=None,\n","                    DisplayFrameNumber=True, DisplaySegmentation=True, ColoredSegmentation=True, DisplayTrackIDs = True):\n","  \"\"\"\n","  Saves the cell paths in a similar format as DisplayTrack displays them\n","  Non-interpolated segments are displayed as \"lime\", interpolated ones are displayed as \"deepskyblue\"\n","\n","  - OutputVideoPath: The path to which the video will be saved\n","  - DisplayFrameNumber: Boolean variable marking whether to display the frame number\n","  - DisplaySegmentation: Boolean variable marking whether to display the segmentations\n","  - DisplayTrackIDs: Boolean variable marking whether to display the track IDs\n","  - Fps: The fps of the saved video\n","  - Starting Frame: The first frame which should be displayed\n","                    Can be useful if tracking was only performed after a certain frame\n","  - Ending Frame: The last frame which should be displayed\n","                  Can be useful if tracking was only performed after a certain frame\n","  - DisplayPeriodicity: How frequently should the frames be displayed.\n","                        Useful for long videos.\n","                        Minimal possible value=1.\n","  \"\"\"\n","  if OutputVideoPath[-4:]!=\".mp4\":\n","    raise Exception(\"Only video paths with mp4 extension are allowed\")\n","\n","  VideoFrames = sorted(os.listdir(VideoPath))\n","  if StartingFrame is None:\n","    StartingFrame = 1\n","  if EndingFrame is None:\n","    EndingFrame = len(VideoFrames)\n","\n","  print(\"Saving Tracks Video\")\n","  try:\n","    ProgressBar = display(progress(0, EndingFrame-StartingFrame), display_id=True)\n","  except:\n","    pass\n","\n","  out = cv2.VideoWriter(OutputVideoPath, cv2.VideoWriter_fourcc(*'mp4v'), Fps, (700,700), True)\n","  for Frame in range(StartingFrame-1,EndingFrame-1):\n","    if np.mod(Frame,DisplayPeriodicity)==0:\n","\n","      Img = cv2.cvtColor(cv2.imread(os.path.join(VideoPath, VideoFrames[Frame])), cv2.COLOR_BGR2RGB)\n","      fig, (ax1) = plt.subplots(1, 1, figsize=(7, 7))\n","      ax1.imshow(Img, cmap=plt.cm.gray, interpolation='nearest')\n","\n","      if DisplaySegmentation:\n","\n","        if ColoredSegmentation:\n","          SegmentsSum = np.zeros_like(Img)\n","          cmap = cm.nipy_spectral\n","          for _, Object in AnnotDF.query(\"Frame == @Frame\").iterrows():\n","              color = cmap((int(Object[\"TrackID\"])*17)%256)\n","              mask = coco_mask.decode(Object[\"SegmentationRLE\"])*255\n","              colored_mask = np.array(np.stack([mask*color[0],mask*color[1],mask*color[2]], axis=2),dtype=np.uint8)\n","              SegmentsSum += colored_mask\n","          ax1.imshow(SegmentsSum, vmax=256, alpha=.3, interpolation='bilinear')\n","\n","        else:\n","          SegmentsSum = np.zeros(np.shape(Img)[0:2])\n","          for _, Object in AnnotDF.query(\"Frame == @Frame\").iterrows():\n","            SegmentsSum += coco_mask.decode(Object[\"SegmentationRLE\"])\n","          ax1.imshow(SegmentsSum, cmap=plt.cm.hot, vmax=2, alpha=.3, interpolation='bilinear')\n","\n","      if DisplayFrameNumber:\n","        ax1.text(1, 10, \"Frame \"+str(Frame+1), color=\"deepskyblue\", fontsize=10)\n","\n","      if DisplayTrackIDs:\n","        for _, Object in AnnotDF.query(\"Frame == @Frame\").iterrows():\n","          color=\"lime\"\n","          if Object[\"Interpolated\"]:\n","            color=\"deepskyblue\"\n","          [cx, cy] = Object[\"Centroid\"]\n","          ax1.text(cx, cy, Object[\"TrackID\"], color=color, fontsize=5, ha='center', va='center')\n","\n","      Img = cv2.cvtColor(fig2data(fig), cv2.COLOR_BGRA2RGB)\n","      out.write(Img)\n","      plt.close(fig)\n","\n","      try:\n","        ProgressBar.update(progress(Frame, EndingFrame-StartingFrame))\n","      except:\n","        pass\n","\n","  out.release()\n","  gc.collect()\n","\n","  try:\n","    ProgressBar.update(progress(1, 1))\n","  except:\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"elapsed":6638058,"status":"ok","timestamp":1713277854402,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"sxDueaVkf_RJ","outputId":"9de7de87-7626-4d24-a83a-becbe6e09d59"},"outputs":[],"source":["#@title Full Tracking\n","\n","AnnotPath = SegmentationSavePath\n","\n","# Performing multiple cell tracking\n","AnnotDF = SingleVideoObjectTracking(TransformedVideoPath,\n","                                    TrackingModelPath,\n","                                    Device,\n","                                    AnnotPath,\n","                                    TimeKernelSize=TimeKernelSize,\n","                                    Color = \"RGB\",\n","                                    Marker = \"BBOX\",\n","                                    SegmentationConfidence = 0.2,\n","                                    MinRequiredSimilarity = 0.2,\n","                                    MaxTimeKernelShift = None)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23795,"status":"ok","timestamp":1713277906084,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"D_tC10wKjP4x","outputId":"b96a6e7b-c187-42ac-ba6a-2ffeebe80af1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successful interpolation of 1018 object instances\n"]}],"source":["#@title Interpolation of Missing Objects\n","\n","# NOTE:\n","# This step is not based on heuristics, and the results are well determined\n","# Also, this step is mandatory for successful Inheritance detection\n","# However it must be performed after Heuristical Post-Processing\n","\n","AnnotDF = InterpolateMissingObjects(AnnotDF)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713277906684,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"M7-Qoy4oo5sM"},"outputs":[],"source":["#@title Saving Track results\n","\n","SaveTracks(AnnotDF,TrackingSavePath)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":984,"status":"ok","timestamp":1713285752137,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"qXlU_T2Npj53"},"outputs":[],"source":["#@title Loading Track results\n","\n","AnnotDF = LoadTracks(TrackingSavePath)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63133,"status":"ok","timestamp":1713277969816,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"LBEknABek-V_","outputId":"d526c1ea-8063-407a-995f-c60ddc57c32d"},"outputs":[{"name":"stdout","output_type":"stream","text":["The track is saved in standard txt format to: \n","./outputs/trackings/600_Tracks.txt\n"]}],"source":["#@title Display and Write Tracking results\n","\n","# Displaying tracking results\n","#DisplayTracks(TransformedVideoPath, AnnotDF)\n","\n","# Writing tracking results to standard format txt\n","WriteTracks(AnnotDF, TrackingWritePath)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":350324,"status":"ok","timestamp":1713286682848,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"3OWtGHlCkKiQ","outputId":"bb093d54-c6a5-4283-822f-c047371d8632"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving Tracks Video\n"]},{"data":{"text/html":["\n","      <progress\n","        value='1'\n","        max='1',\n","        style='width: 10%'\n","      >\n","        1\n","      </progress>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Saving tracking result to video\n","SaveTracksVideo(TransformedVideoPath, AnnotDF, TrackingVideoPath, Fps=30, ColoredSegmentation=True, DisplayTrackIDs=False)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22325,"status":"ok","timestamp":1713256593868,"user":{"displayName":"Gergely Szabó","userId":"12741425224045507482"},"user_tz":-120},"id":"813ei6O9ahhY","outputId":"b03f182f-d833-432c-f9e1-b34d4716bf3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqhCoKClDrnu"},"outputs":[],"source":["SaveTracks(AnnotDF, \"/content/gdrive/MyDrive/General Symmetric Tracking/MOTSynth - CVPR22/codes/refactored predictor/outputs/\"+sample_record_name+\"_Tracks.json\")\n","SaveTracksVideo(TransformedVideoPath, AnnotDF, \"/content/gdrive/MyDrive/General Symmetric Tracking/MOTSynth - CVPR22/codes/refactored predictor/outputs/\"+sample_record_name+\"_Tracks.mp4\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vqMs8aa0hhY"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1yAb-Cu4AcPdbFsU_OR_5rscIZaGri5lX","timestamp":1713204454077}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
