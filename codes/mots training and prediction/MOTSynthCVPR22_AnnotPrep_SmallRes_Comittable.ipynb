{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3_MUZCDX2cFv"},"outputs":[],"source":["import json\n","import numpy as np\n","import cv2\n","import pycocotools.mask as maskUtils\n","import os\n","from time import time\n","import multiprocessing\n","import traceback\n","import shutil\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8uPrHyN2xVB"},"outputs":[],"source":["# Train samples: 1: [0-100), 2: [100-200), 3: [200-300), 4:, [300-400), 5: [400-500), 6: [500-600)\n","# Test samples: 7: [600-700), 8: [700-768)\n","\n","sample_names = list(map(lambda x: str(x).zfill(3), range(0,100)))\n","sample_range_str = f\"[{sample_names[0]},{sample_names[-1]}]\"\n","\n","input_root = \"/path/to/original/mots/data/\"\n","output_root = \"/output/path/\"\n","max_allowed_multiprocesses = 16\n","\n","resize_factor = 0.25\n","min_threshold_ratio = 0.0005\n","\n","annot_dir = os.path.join(input_root, \"annotations/\")\n","recording_dir = os.path.join(input_root, \"recordings/\")\n","\n","os.makedirs(os.path.join(output_root,\"annotations\"), exist_ok=True)\n","os.makedirs(os.path.join(output_root,\"recordings\"), exist_ok=True)\n","os.makedirs(os.path.join(output_root,\"logs\"), exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wH3Vjog7GIj"},"outputs":[],"source":["message_logfile_name = os.path.join(output_root,\"logs\",sample_range_str+\"annot\"+\"_message_log.txt\")\n","error_logfile_name = os.path.join(output_root,\"logs\",sample_range_str+\"annot\"+\"_error_log.txt\")\n","corrupt_samples_logfile_name = os.path.join(output_root,\"logs\",sample_range_str+\"annot\"+\"_corrupt_samples.txt\")\n","open(message_logfile_name, 'w').close()\n","open(error_logfile_name, 'w').close()\n","open(corrupt_samples_logfile_name, 'w').close()\n","\n","num_processes = multiprocessing.cpu_count()\n","with open(message_logfile_name, 'a') as f:\n","  f.write(f\"cpu count: {num_processes}\\n\")\n","  f.write(f\"max allowed processes: {max_allowed_multiprocesses}\\n\")\n","\n","def process_annot(sample_name, recording_dir, annot_dir, output_dir, resize_factor, min_threshold_ratio, error_logfile_name):\n","  try:\n","    os.makedirs(os.path.join(output_dir,\"recordings\",\"frames\",sample_name,\"rgb\"), exist_ok=True)\n","    OriginalAnnot = None\n","    TransformedAnnot = {}\n","\n","    t1 = time()\n","    with open(annot_dir+sample_name+\".json\") as json_file:\n","      OriginalAnnot = json.load(json_file)\n","    t2 = time()\n","\n","    TransformedAnnot[\"info\"] = OriginalAnnot[\"info\"]\n","    TransformedAnnot[\"info\"][\"img_height\"]=int(TransformedAnnot[\"info\"][\"img_height\"]*resize_factor)\n","    TransformedAnnot[\"info\"][\"img_width\"]=int(TransformedAnnot[\"info\"][\"img_width\"]*resize_factor)\n","    H = TransformedAnnot[\"info\"][\"img_height\"]\n","    W = TransformedAnnot[\"info\"][\"img_width\"]\n","\n","    TransformedAnnot[\"licenses\"] = OriginalAnnot[\"licenses\"]\n","\n","    TransformedAnnot[\"images\"] = OriginalAnnot[\"images\"]\n","    for i in range(len(TransformedAnnot[\"images\"])):\n","      TransformedAnnot[\"images\"][i]['height'] = H\n","      TransformedAnnot[\"images\"][i]['width'] = W\n","\n","    TransformedAnnot[\"categories\"] = [{\"id\":  1, \"name\": \"person\", \"supercategory\": \"person\"}]\n","\n","    TransformedAnnot[\"annotations\"] = []\n","\n","    AnnotFrameDict = {}\n","    for item in OriginalAnnot['annotations']:\n","        image_id = item['image_id']\n","        if image_id not in AnnotFrameDict:\n","            AnnotFrameDict[image_id] = []\n","        AnnotFrameDict[image_id].append(item)\n","\n","    for frame in range(len(AnnotFrameDict)):\n","\n","      img_annot = AnnotFrameDict[int(sample_name+str(frame).zfill(4))]\n","\n","      accepted_annots = []\n","\n","      for obj in img_annot:\n","\n","        segmentation = maskUtils.decode(obj[\"segmentation\"])\n","        segmentation = cv2.resize(segmentation, (W, H))\n","\n","        sum_segm = np.sum(segmentation)\n","        if sum_segm>H*W*min_threshold_ratio:\n","\n","          segmentation_RLE = maskUtils.encode(np.asarray(segmentation, order=\"F\", dtype=\"uint8\"))\n","          segmentation_RLE[\"counts\"] = segmentation_RLE[\"counts\"].decode('utf-8')\n","\n","          x, y, w, h = cv2.boundingRect(segmentation)\n","\n","          accepted_annot = {\"segmentation\": segmentation_RLE,\n","                            \"iscrowd\": obj[\"iscrowd\"],\n","                            \"image_id\": obj[\"image_id\"],\n","                            \"category_id\": obj[\"category_id\"],\n","                            \"id\": obj[\"id\"],\n","                            \"bbox\": [int(x),int(y),int(w),int(h)],\n","                            \"area\": int(np.sum(segmentation)),\n","                            \"ped_id\": obj[\"ped_id\"]}\n","          accepted_annots.append(accepted_annot)\n","\n","      TransformedAnnot[\"annotations\"].extend(accepted_annots)\n","\n","    t3 = time()\n","    with open(os.path.join(output_dir, \"annotations\", sample_name+\".json\"), 'w') as json_file:\n","      json.dump(TransformedAnnot, json_file)\n","\n","    t4 = time()\n","    with open(message_logfile_name, 'a') as f:\n","      f.write(f\"<- Sample {sample_name} |\\tLoad: {t2 - t1:.2f} s\\tTf: {t3 - t2:.2f} s\\tSave: {t4 - t3:.2f} s\\n\")\n","\n","  except Exception:\n","    with open(error_logfile_name, 'a') as f:\n","      f.write(f\"sample error\\t[{sample_name}]\\n{traceback.format_exc()}\\n\")\n","    with open(corrupt_samples_logfile_name, 'a') as f:\n","      f.write(f\"{sample_name}\\n\")\n","    try:\n","      shutil.rmtree(os.path.join(output_dir,\"recordings\",\"frames\",sample_name))\n","    except: pass\n","    try:\n","      os.remove(os.path.join(output_dir, \"annotations\", sample_name+\".json\"))\n","    except: pass\n","\n","def process_annot_wrapper(sample_name):\n","  with open(message_logfile_name, 'a') as f:\n","    f.write(f\"-> Sample {sample_name} in pool\\n\")\n","  process_annot(sample_name, recording_dir, annot_dir, output_root, resize_factor, min_threshold_ratio, error_logfile_name)\n","\n","\n","num_required_processes = min([len(sample_names), num_processes, max_allowed_multiprocesses])\n","pool = multiprocessing.Pool(processes=num_required_processes)\n","chunk_size = len(sample_names) // num_required_processes\n","pool.map(process_annot_wrapper, sample_names, chunksize=chunk_size)\n","pool.close()\n","pool.join()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPQXe467fMDAGdLGevDG4M8","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
