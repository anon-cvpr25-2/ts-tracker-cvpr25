{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZnDASrYeNoc"},"outputs":[],"source":["import sys\n","import numpy as np\n","import cv2\n","import os\n","from PIL import Image\n","import time\n","import random\n","import multiprocessing\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import transforms as T\n","#from torchvision.transforms import v2 as T\n","\n","import segmentation_models_pytorch as smp\n","\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyTx2256eslG"},"outputs":[],"source":["tracking_range = 2\n","epochs = 4\n","warm_restarts_per_epoch = 5\n","scheduler_switch_epochs = 3\n","backbone = \"resnet50\"\n","model_save_steps = 10000\n","resume = False\n","max_workers = 8\n","\n","data_root = \"/path/to/transformed/train/data/train/recordings/frames/\"\n","model_name = f\"MOTSynth_KernelTracker_[DLV3p,{backbone}]_FBtr{tracking_range}_Ep{epochs}_Adv2\"\n","output_root = \"/model/save/dir/\"\n","\n","output_dir = None\n","if os.path.exists(output_root) and os.path.isdir(output_root):\n","  output_dir = os.path.join(output_root, f\"DLV3p_{backbone}_FBtr{tracking_range}_Ep{epochs}\")\n","  os.makedirs(output_dir, exist_ok=True)\n","else:\n","  raise FileNotFoundError(f\"The directory '{output_root}' does not exist.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViEHpw-ki9bB"},"outputs":[],"source":["num_workers = min(max_workers, multiprocessing.cpu_count())\n","print(f\"Used CPU workers for dataloading: {num_workers}\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3YermdkkCWu"},"outputs":[],"source":["# Utility functions and definitions\n","\n","def remove_extension(file_list):\n","    return [file.split('.')[0] for file in file_list]\n","\n","def format_duration(seconds):\n","    days, remainder = divmod(seconds, 86400)\n","    hours, remainder = divmod(remainder, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","\n","    formatted_time = f\"{int(days)} days {int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n","    return formatted_time\n","\n","class InvalidDataError(Exception):\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb0Eu7RGm9OA"},"outputs":[],"source":["# Defining dataset and color augmentations\n","\n","class MOTSynthLocalTrackingDataset(Dataset):\n","    def __init__(self, data_root, tracking_range, video_len = 1800, color_transforms=None, min_central_obj_threshold_ratio = 0.0005):\n","        self.data_root = data_root\n","        valid_sample_names = self.__RegisterValidSamples__(data_root, video_len)\n","        print(\"Registered Samples: \")\n","        print('\\n'.join(map(lambda name: f'- {name}', valid_sample_names)))\n","        self.sample_names = list(valid_sample_names)\n","        self.tracking_range = tracking_range\n","        self.video_len = video_len\n","        self.color_transforms = color_transforms\n","        self.min_central_obj_threshold_ratio = min_central_obj_threshold_ratio\n","        print(f\"Amount of available frames: {len(self.sample_names)*self.video_len}\\n\")\n","\n","    def __RegisterValidSamples__(self, data_root, video_len = None):\n","        sample_names = set(os.listdir(data_root))\n","        valid_sample_names = set()\n","        for sample_name in sample_names:\n","          sample_path = os.path.join(data_root, sample_name)\n","          sample_path_folders = os.listdir(sample_path)\n","          if (\"annot_IDs\" in sample_path_folders) and (\"rgb\" in sample_path_folders):\n","            annots = set(remove_extension(os.listdir(os.path.join(sample_path, \"annot_IDs\"))))\n","            rgbs = set(remove_extension(os.listdir(os.path.join(sample_path, \"rgb\"))))\n","            if annots == rgbs and (video_len is None or len(rgbs) == video_len):\n","              valid_sample_names.add(sample_name)\n","        return valid_sample_names\n","\n","    def __len__(self):\n","        return len(self.sample_names)*self.video_len\n","\n","    def __getitem__(self, idx):\n","      video_name = self.sample_names[int(idx/self.video_len)]\n","      frame = idx%self.video_len+1\n","      central_img = cv2.imread(os.path.join(self.data_root,video_name,\"rgb\", \"{:04d}\".format(frame)+\".jpg\"))\n","      H, W, C = np.shape(central_img)\n","      ret_val = True\n","\n","      try:\n","\n","        input = [np.zeros([H,W,C])]*(2*self.tracking_range+1)\n","        annot = [np.zeros([H,W])]*(2*self.tracking_range+1)\n","        input[self.tracking_range] = central_img\n","\n","        # Forward loop from center (to handle forward temporal edges)\n","        for dt in range(0, self.tracking_range+1):\n","          if frame+dt<=self.video_len and dt != 0:\n","            input[self.tracking_range+dt] = cv2.imread(os.path.join(self.data_root,video_name,\"rgb\",\n","                                                                    \"{:04d}\".format(frame+dt)+\".jpg\"))\n","          else:\n","            input[self.tracking_range+dt] = input[self.tracking_range]\n","\n","          if frame+dt<=self.video_len:\n","            annot[self.tracking_range+dt] = np.array(Image.open(os.path.join(self.data_root,video_name,\"annot_IDs\",\n","                                                                                \"{:04d}\".format(frame+dt)+\".png\")))\n","          else:\n","            annot[self.tracking_range+dt] = annot[self.tracking_range]\n","\n","        # Backward loop from center (to handle backward temporal edges)\n","        for dt in reversed(range(-self.tracking_range,1)):\n","          if frame+dt > 0 and dt != 0:\n","            input[self.tracking_range+dt] = cv2.imread(os.path.join(self.data_root,video_name,\"rgb\",\n","                                                                    \"{:04d}\".format(frame+dt)+\".jpg\"))\n","          else:\n","            input[self.tracking_range+dt] = input[self.tracking_range]\n","\n","          if frame+dt > 0 and dt != 0:\n","            annot[self.tracking_range+dt] = np.array(Image.open(os.path.join(self.data_root,video_name,\"annot_IDs\",\n","                                                                                \"{:04d}\".format(frame+dt)+\".png\")))\n","          else:\n","            annot[self.tracking_range+dt] = annot[self.tracking_range]\n","\n","        input = np.array(input)\n","        annot = np.array(annot)\n","\n","        # Select object randomly, but only one with sufficient amount of pixels in central image\n","        unique_values = np.unique(annot[self.tracking_range])\n","        unique_values = unique_values[unique_values != 0]\n","        valid_object_id = None\n","        if len(unique_values)>0:\n","          random.shuffle(unique_values)\n","          for object_id in unique_values:\n","            central_img_obj = np.array(annot[self.tracking_range] == object_id, dtype=int)\n","            if np.sum(central_img_obj) > H*W*self.min_central_obj_threshold_ratio:\n","              valid_object_id = object_id\n","              break\n","        else:\n","          raise InvalidDataError(f\"Data with no objects at index {idx}\")\n","        if valid_object_id is not None:\n","          label = annot == valid_object_id\n","        else:\n","          raise InvalidDataError(f\"Data with no unobscured objects at index {idx}\")\n","\n","        # Merge temporal and color dimensions, to get from [T,H,W,C] to [H, W, T combined C]\n","        input = input.transpose(1, 2, 0, 3).reshape(H, W, -1)\n","        label = np.transpose(label, (1, 2, 0))\n","\n","        # Perform color augmentations (positional augmentations are not available for now)\n","        if self.color_transforms:\n","          for i in range(np.shape(label)[2]):\n","            input[:,:,3*i:3*(i+1)] = self.color_transforms(Image.fromarray(input[:,:,3*i:3*(i+1)]))\n","\n","        # Mark object on last input channel with its solid bounding box (not centroid marking as it may be outside of the object)\n","        input = np.array(input, dtype=np.uint8)\n","        annot = np.array(annot, dtype=np.uint8)\n","        bx, by, bw, bh = cv2.boundingRect(np.array(label[:,:,self.tracking_range], dtype=np.uint8)*255)\n","        bounding_rect = cv2.rectangle(np.zeros([H,W]), (bx, by), (bx + bw, by + bh), 255, thickness=cv2.FILLED)\n","        input = np.concatenate([input, np.expand_dims(bounding_rect, axis=2)], axis=2)\n","\n","        # Transform the data from [H, W, C] to [C, H, W] and into float Torch tensors\n","        input = torch.tensor(input, dtype=torch.float32) / 255.0\n","        label = torch.tensor(label, dtype=torch.float32)\n","        input = input.permute(2,0,1)\n","        label = label.permute(2,0,1)\n","\n","      except Exception as e:\n","        print(f\"Error during data loading: {e}\")\n","        input = torch.zeros([C*(2*self.tracking_range+1)+1, H, W], dtype=torch.float32)\n","        label = torch.zeros([2*self.tracking_range+1, H, W], dtype=torch.float32)\n","        ret_val = False\n","\n","      # Pad the data to 16 divisible shape for the deeplabv3+ architecture\n","      pad_h = (16 - H % 16) % 16\n","      pad_w = (16 - W % 16) % 16\n","      input = F.pad(input, (0, pad_w, 0, pad_h), mode='constant', value=0)\n","      label = F.pad(label, (0, pad_w, 0, pad_h), mode='constant', value=0)\n","\n","      return input, label, ret_val\n","\n","ColorTransforms = T.RandomApply(torch.nn.ModuleList([\n","    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.02)\n","]), p=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"520ZFpg50Rd5"},"outputs":[],"source":["# SMP architecture (up to choice, but probably DeepLabV3+)\n","\n","SMPModel = smp.DeepLabV3Plus(\n","    encoder_name=backbone,\n","    encoder_weights=\"imagenet\",\n","    in_channels=3*(2*tracking_range+1)+1,\n","    classes=2*tracking_range+1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Ok7jJ2oz8P4"},"outputs":[],"source":["# Setting up training parameters and environment, loading the training data\n","\n","batch_size = 4\n","loss_disp_period = 10\n","loss_function = nn.BCEWithLogitsLoss()\n","\n","train_loader = DataLoader(MOTSynthLocalTrackingDataset(data_root, tracking_range = tracking_range, color_transforms = ColorTransforms, min_central_obj_threshold_ratio = 0.0005),\n","                          batch_size=batch_size,\n","                          shuffle=True,\n","                          num_workers=num_workers)\n","iters = len(train_loader)\n","\n","if resume:\n","  with open(os.path.join(output_dir,\"last_model.txt\"), \"r\") as f:\n","    last_model_name = f.read()\n","    SMPModel = torch.load(os.path.join(output_dir,last_model_name))\n","else:\n","  with open(os.path.join(output_dir,model_name+\"_log.txt\"), \"w\"): pass\n","\n","SMPModel.to(device)\n","optimizer = optim.SGD(SMPModel.parameters(), lr=0.1, weight_decay=1e-5)\n","scheduler1 = CosineAnnealingWarmRestarts(optimizer, T_0=int(iters/warm_restarts_per_epoch))\n","scheduler2 = CosineAnnealingLR(optimizer, T_max = (epochs-scheduler_switch_epochs)*iters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfAENe4s2awR"},"outputs":[],"source":["# Training the model\n","\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  running_time = 0.0\n","  t = time.time()\n","  for i, data in enumerate(train_loader, 0):\n","    current_step = epoch*iters+i\n","\n","    if all(data[2]):\n","      inputs, ground_truth = data[0], data[1]\n","\n","      inputs=inputs.to(device)\n","      ground_truth=ground_truth.to(device)\n","\n","      optimizer.zero_grad()\n","      output = SMPModel(inputs)\n","      loss = loss_function(output, ground_truth)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","\n","    if current_step<=scheduler_switch_epochs*iters:\n","      scheduler1.step()\n","    else:\n","      scheduler2.step()\n","\n","    running_time += time.time()-t\n","    t = time.time()\n","\n","    # print statistics\n","    if current_step!=0 and current_step % loss_disp_period == 0:\n","      step_time = running_time/loss_disp_period\n","      full_steps = iters*epochs\n","      time_estimate = (full_steps-current_step)*step_time\n","      lr = optimizer.param_groups[0]['lr']\n","      print(f'eta: {format_duration(time_estimate)}, t_step: {step_time:.2f} sec, ep: {epoch + 1}, iter: {current_step}/{full_steps}, lr: {lr:.2g}, loss: {running_loss/loss_disp_period:.2g}')\n","      with open(os.path.join(output_dir,model_name+\"_log.txt\"), \"a+\") as LogFile:\n","        LogFile.write(f'eta: {format_duration(time_estimate)}, t_step: {step_time:.2f} sec, ep: {epoch + 1}, iter: {current_step}/{full_steps}, lr: {lr:.2g}, loss: {running_loss/loss_disp_period:.2g}\\n')\n","      running_loss = 0.0\n","      running_time = 0.0\n","      gc.collect()\n","\n","    if current_step!=0 and current_step % model_save_steps == 0:\n","      torch.save(SMPModel, os.path.join(output_dir,f\"{model_name}_{current_step}.pth\"))\n","      with open(os.path.join(output_dir,\"last_model.txt\"), \"w\") as f:\n","        f.write(f\"{model_name}_{current_step}.pth\")\n","\n","torch.save(SMPModel, os.path.join(output_dir,f\"{model_name}_final.pth\"))\n","with open(os.path.join(output_dir,\"last_model.txt\"), \"w\") as f:\n","  f.write(f\"{model_name}_final.pth\")\n","\n","print('Finished Training of SMP model')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOiC5OH/u0At63dlMDXMgTF","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
