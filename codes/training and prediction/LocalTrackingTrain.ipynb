{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ma6wxtIthkLt"},"outputs":[],"source":["import sys\n","import numpy as np\n","import cv2\n","import os\n","from PIL import Image\n","import time\n","import random\n","import multiprocessing\n","import re\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import transforms as T\n","#from torchvision.transforms import v2 as T\n","\n","import segmentation_models_pytorch as smp\n","\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"deWltxAWhnaE"},"outputs":[],"source":["tracking_range = 4\n","train_percentage = 1.0\n","epochs = 50\n","warm_restarts_per_epoch = 2\n","scheduler_switch_epochs = 8\n","backbone = \"resnet50\"\n","model_save_steps = 1000\n","resume = False\n","max_workers = 8\n","\n","sample_category = \"ArrowSynthTurns\"\n","data_root = f\"/path/to/train/samples/{sample_category}_train_samples\"\n","sample_names = sorted(os.listdir(data_root))\n","sample_names = sample_names[:int(len(sample_names) * train_percentage)]\n","\n","model_name = f\"{sample_category}_[DLV3p,{backbone}]_FBtr{tracking_range}_Ep{epochs}_Adv2_SR{train_percentage}\"\n","output_root = \"/output/dir/\"\n","\n","output_dir = None\n","if os.path.exists(output_root) and os.path.isdir(output_root):\n","  output_dir = os.path.join(output_root, f\"{sample_category}/LocalTracking/{sample_category}_DLV3p_{backbone}_FBtr{tracking_range}_Ep{epochs}_SR{train_percentage}\")\n","  os.makedirs(output_dir, exist_ok=True)\n","else:\n","  raise FileNotFoundError(f\"The directory '{output_root}' does not exist.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1NbMqwXiKbf"},"outputs":[],"source":["num_workers = min(max_workers, multiprocessing.cpu_count())\n","print(f\"Used CPU workers for dataloading: {num_workers}\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvIGOV8oiOEh"},"outputs":[],"source":["def remove_extension(file_list):\n","    return [file.split('.')[0] for file in file_list]\n","\n","def format_duration(seconds):\n","    days, remainder = divmod(seconds, 86400)\n","    hours, remainder = divmod(remainder, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","\n","    formatted_time = f\"{int(days)} days {int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n","    return formatted_time\n","\n","class InvalidDataError(Exception):\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pv7c-IabhadZ"},"outputs":[],"source":["# Defining dataset and color augmentations\n","\n","class CTCLocalTrackingDataset(Dataset):\n","    def __init__(self, data_root, sample_names, tracking_range, color_transforms=None):\n","        self.data_root = data_root\n","        self.samples_list = self.__RegisterValidSamples__(data_root, sample_names)\n","        print(\"Registered Samples: \")\n","        for i in range(len(self.samples_list)):\n","          print(f\"sample: {self.samples_list[i]['sample_name']}, num_frames: {self.samples_list[i]['num_frames']}\")\n","        self.tracking_range = tracking_range\n","        self.color_transforms = color_transforms\n","        #print(f\"Amount of available frames: {len(self.sample_names)*self.video_len}\\n\")\n","\n","    def __RegisterValidSamples__(self, data_root, sample_names):\n","      samples_list = []\n","      for sample_name in sample_names:\n","        video_folder = os.path.join(data_root, sample_name, 'imgs')\n","        gt_folder = os.path.join(data_root, sample_name, 'labels')\n","\n","        image_files = sorted([file for file in os.listdir(video_folder) if file.endswith('.png')])\n","        IDs = sorted([file[0:7] for file in image_files])\n","\n","        track_files = sorted([file for file in os.listdir(gt_folder) if file.endswith('.png')])\n","        track_IDs = sorted([file[0:7] for file in track_files])\n","        if len(set(IDs)-set(track_IDs)) == 0:\n","          sample_dict = {\"sample_name\": sample_name, \"num_frames\": len(IDs), \"image_files\": image_files, \"annot_files\": track_files}\n","          samples_list.append(sample_dict)\n","        else:\n","          print(f\"Sample {sample_name} does not contain all the necessary tracking annotations, and thus it is not registered!\")\n","      return samples_list\n","\n","    def __len__(self):\n","      all_samples = 0\n","      for i in range(len(self.samples_list)):\n","        all_samples += self.samples_list[i]['num_frames']\n","      return all_samples\n","\n","    def getitem(self,idx):\n","      return self.__getitem__(idx)\n","\n","    def __getitem__(self, idx):\n","      frame = idx\n","      for sample_id, sample in enumerate(self.samples_list):\n","        if frame < sample['num_frames']:\n","            break\n","        else:\n","            frame -= sample['num_frames']\n","      sample = self.samples_list[sample_id]\n","\n","      central_img = Image.open(os.path.join(self.data_root, sample['sample_name'], 'imgs', sample['image_files'][frame]))\n","      grayscale = False\n","      if len(np.shape(central_img)) == 2:\n","        grayscale = True\n","        central_img = np.expand_dims(central_img, axis=2)\n","      H, W, C = np.shape(central_img)\n","      ret_val = True\n","\n","      try:\n","\n","        input = [np.zeros([H,W,C])]*(2*self.tracking_range+1)\n","        annot = [np.zeros([H,W])]*(2*self.tracking_range+1)\n","        input[self.tracking_range] = central_img\n","\n","        # Forward loop from center (to handle forward temporal edges)\n","        for dt in range(0, self.tracking_range+1):\n","          if frame+dt<sample['num_frames'] and dt != 0:\n","            img = np.array(Image.open(os.path.join(self.data_root, sample['sample_name'], 'imgs', sample['image_files'][frame+dt])))\n","            if grayscale:\n","              img = np.expand_dims(img, axis=2)\n","            input[self.tracking_range+dt] = img\n","\n","          else:\n","            input[self.tracking_range+dt] = input[self.tracking_range]\n","\n","          if frame+dt<sample['num_frames']:\n","            annot[self.tracking_range+dt] = np.array(Image.open(os.path.join(self.data_root, sample['sample_name'], 'labels', sample['annot_files'][frame+dt])))\n","\n","          else:\n","            annot[self.tracking_range+dt] = annot[self.tracking_range]\n","\n","        # Backward loop from center (to handle backward temporal edges)\n","        for dt in reversed(range(-self.tracking_range,1)):\n","          if frame+dt >= 0 and dt != 0:\n","            img = np.array(Image.open(os.path.join(self.data_root, sample['sample_name'], 'imgs', sample['image_files'][frame+dt])))\n","            if grayscale:\n","              img = np.expand_dims(img, axis=2)\n","            input[self.tracking_range+dt] = img\n","\n","          else:\n","            input[self.tracking_range+dt] = input[self.tracking_range]\n","\n","          if frame+dt >= 0 and dt != 0:\n","            annot[self.tracking_range+dt] = np.array(Image.open(os.path.join(self.data_root, sample['sample_name'], 'labels', sample['annot_files'][frame+dt])))\n","          else:\n","            annot[self.tracking_range+dt] = annot[self.tracking_range]\n","\n","        input = np.array(input)\n","        annot = np.array(annot)\n","\n","        # Select object randomly\n","        unique_values = np.unique(annot[self.tracking_range])\n","        unique_values = unique_values[unique_values != 0]\n","        object_id = None\n","        if len(unique_values)>0:\n","          random.shuffle(unique_values)\n","          object_id = unique_values[0]\n","\n","        else:\n","          raise InvalidDataError(f\"Data with no objects at index {idx}\")\n","        if object_id is not None:\n","          label = annot == object_id\n","        else:\n","          raise InvalidDataError(f\"Data with no unobscured objects at index {idx}\")\n","\n","        # Merge temporal and color dimensions, to get from [T,H,W,C] to [H, W, T combined C]\n","        input = input.transpose(1, 2, 0, 3).reshape(H, W, -1)\n","        label = np.transpose(label, (1, 2, 0))\n","\n","        # Perform color augmentations (positional augmentations are not available for now)\n","        if self.color_transforms:\n","          for i in range(np.shape(label)[2]):\n","            if grayscale:\n","              input[:,:,i:i+1] = np.expand_dims(self.color_transforms(Image.fromarray(np.squeeze(input[:,:,i:i+1], axis = 2))), axis = 2)\n","            else:\n","              input[:,:,3*i:3*(i+1)] = self.color_transforms(Image.fromarray(input[:,:,3*i:3*(i+1)]))\n","\n","        # Mark object on last input channel with its solid bounding box (not centroid marking as it may be outside of the object)\n","        input = np.array(input, dtype=np.uint8)\n","        annot = np.array(annot, dtype=np.uint8)\n","        bx, by, bw, bh = cv2.boundingRect(np.array(label[:,:,self.tracking_range], dtype=np.uint8)*255)\n","        bounding_rect = cv2.rectangle(np.zeros([H,W]), (bx, by), (bx + bw, by + bh), 255, thickness=cv2.FILLED)\n","        input = np.concatenate([input, np.expand_dims(bounding_rect, axis=2)], axis=2)\n","\n","        # Transform the data from [H, W, C] to [C, H, W] and into float Torch tensors\n","        input = torch.tensor(input, dtype=torch.float32) / 255.0\n","        label = torch.tensor(label, dtype=torch.float32)\n","        input = input.permute(2,0,1)\n","        label = label.permute(2,0,1)\n","\n","      except Exception as e:\n","        print(f\"Error during data loading: {e}\")\n","        input = torch.zeros([C*(2*self.tracking_range+1)+1, H, W], dtype=torch.float32)\n","        label = torch.zeros([2*self.tracking_range+1, H, W], dtype=torch.float32)\n","        ret_val = False\n","\n","\n","      # Pad the data to 16 divisible shape for the deeplabv3+ architecture\n","      pad_h = (16 - H % 16) % 16\n","      pad_w = (16 - W % 16) % 16\n","      input = F.pad(input, (0, pad_w, 0, pad_h), mode='constant', value=0)\n","      label = F.pad(label, (0, pad_w, 0, pad_h), mode='constant', value=0)\n","\n","      return input, label, ret_val\n","\n","ColorTransforms = T.RandomApply(torch.nn.ModuleList([\n","    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.02)\n","]), p=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hjWD-UNsxhI"},"outputs":[],"source":["# SMP architecture (up to choice, but probably DeepLabV3+)\n","\n","SMPModel = smp.DeepLabV3Plus(\n","    encoder_name=backbone,\n","    encoder_weights=\"imagenet\",\n","    in_channels=(2*tracking_range+1)+1,\n","    classes=2*tracking_range+1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrfZsvXh-nf0"},"outputs":[],"source":["batch_size = 8\n","loss_disp_period = 10\n","loss_function = nn.BCEWithLogitsLoss()\n","\n","train_loader = DataLoader(CTCLocalTrackingDataset(data_root, sample_names, tracking_range = tracking_range, color_transforms = ColorTransforms),\n","                          batch_size=batch_size,\n","                          shuffle=True,\n","                          num_workers=num_workers)\n","iters = len(train_loader)\n","\n","if resume:\n","  with open(os.path.join(output_dir,\"last_model.txt\"), \"r\") as f:\n","    last_model_name = f.read()\n","    SMPModel = torch.load(os.path.join(output_dir,last_model_name))\n","else:\n","  with open(os.path.join(output_dir,model_name+\"_log.txt\"), \"w\"): pass\n","\n","SMPModel.to(device)\n","optimizer = optim.SGD(SMPModel.parameters(), lr=0.1, weight_decay=1e-5)\n","scheduler1 = CosineAnnealingWarmRestarts(optimizer, T_0=int(iters/warm_restarts_per_epoch))\n","scheduler2 = CosineAnnealingLR(optimizer, T_max = (epochs-scheduler_switch_epochs)*iters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYHLMzVKrUBl"},"outputs":[],"source":["# Training the model\n","\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  running_time = 0.0\n","  t = time.time()\n","  for i, data in enumerate(train_loader, 0):\n","    current_step = epoch*iters+i\n","\n","    if all(data[2]):\n","      inputs, ground_truth = data[0], data[1]\n","\n","      inputs=inputs.to(device)\n","      ground_truth=ground_truth.to(device)\n","\n","      optimizer.zero_grad()\n","      output = SMPModel(inputs)\n","      loss = loss_function(output, ground_truth)\n","      loss.backward()\n","      optimizer.step()\n","\n","      running_loss += loss.item()\n","\n","    if current_step<=scheduler_switch_epochs*iters:\n","      scheduler1.step()\n","    else:\n","      scheduler2.step()\n","\n","    running_time += time.time()-t\n","    t = time.time()\n","\n","    # print statistics\n","    if current_step!=0 and current_step % loss_disp_period == 0:\n","      step_time = running_time/loss_disp_period\n","      full_steps = iters*epochs\n","      time_estimate = (full_steps-current_step)*step_time\n","      lr = optimizer.param_groups[0]['lr']\n","      print(f'eta: {format_duration(time_estimate)}, t_step: {step_time:.2f} sec, ep: {epoch + 1}, iter: {current_step}/{full_steps}, lr: {lr:.2g}, loss: {running_loss/loss_disp_period:.2g}')\n","      with open(os.path.join(output_dir,model_name+\"_log.txt\"), \"a+\") as LogFile:\n","        LogFile.write(f'eta: {format_duration(time_estimate)}, t_step: {step_time:.2f} sec, ep: {epoch + 1}, iter: {current_step}/{full_steps}, lr: {lr:.2g}, loss: {running_loss/loss_disp_period:.2g}\\n')\n","      running_loss = 0.0\n","      running_time = 0.0\n","      gc.collect()\n","\n","    if current_step!=0 and current_step % model_save_steps == 0:\n","      torch.save(SMPModel, os.path.join(output_dir,f\"{model_name}_{current_step}.pth\"))\n","      with open(os.path.join(output_dir,\"last_model.txt\"), \"w\") as f:\n","        f.write(f\"{model_name}_{current_step}.pth\")\n","\n","torch.save(SMPModel, os.path.join(output_dir,f\"{model_name}_final.pth\"))\n","with open(os.path.join(output_dir,\"last_model.txt\"), \"w\") as f:\n","  f.write(f\"{model_name}_final.pth\")\n","\n","print('Finished Training of SMP model')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNT5IMnsMOa32xXxoWSFEL2","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
